{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ke3trKY329OE"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==0.16.2\n",
    "!pip install opencv-python\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from object_detection_utils import *\n",
    "import random\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnMSQyng3WZz"
   },
   "source": [
    "# Download the dataset\n",
    "First, let's download the dataset. It consists of images of plant, bounding box annotations, and leaf counts annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8649,
     "status": "ok",
     "timestamp": 1587634455959,
     "user": {
      "displayName": "David Rapado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhgGc-mH0zw_o5dbPloHXmJzhZNYAiWquZRqPkF=s64",
      "userId": "18266457376573216970"
     },
     "user_tz": -120
    },
    "id": "jCI1N4a-20hj",
    "outputId": "d145eab8-a991-4fde-9a1f-9f45dd284792"
   },
   "outputs": [],
   "source": [
    "!git clone https://git.wur.nl/deep-learning-course/leaf-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hemjFB873q4Q"
   },
   "source": [
    "Let's have a look at one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8490,
     "status": "ok",
     "timestamp": 1587634456489,
     "user": {
      "displayName": "David Rapado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhgGc-mH0zw_o5dbPloHXmJzhZNYAiWquZRqPkF=s64",
      "userId": "18266457376573216970"
     },
     "user_tz": -120
    },
    "id": "NvRgFdLf3uWB",
    "outputId": "fe49fe00-2ff4-4d35-8cd0-3e64f505b548"
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"leaf-dataset/detection/ara2012_plant001_rgb.png\")\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvbG79sc5jxc"
   },
   "source": [
    "# The `Dataset`class\n",
    "Now, let's create a dataset customized to our data.\n",
    "We will call it `LeafDetectionDataset`. \n",
    "\n",
    "\n",
    "**Exercise:** complete the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpbTQUxe3Nev"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LeafDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, img_size, is_train=True, transforms=None):\n",
    "        \"\"\"\n",
    "        Constructor of the LeafDetectionDataset\n",
    "        :param root: the root folder of the dataset\n",
    "        :param is_train: Whether to return the training or test set. Default: True.\n",
    "        :param transforms: list of transformations to be applied to the data\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.resize = ResizeWithBBox(img_size)\n",
    "\n",
    "        imgs = glob.glob(os.path.join(root, \"*rgb.png\"))\n",
    "        self.img_files = imgs.copy()\n",
    "        bboxes = glob.glob(os.path.join(root, \"*bbox.csv\"))\n",
    "        imgs.sort()\n",
    "        bboxes.sort()\n",
    "\n",
    "        # Split the data into train and validation.\n",
    "        x_train, x_test, y_train, y_test = train_test_split(imgs, bboxes, test_size=0.2, random_state=42)\n",
    "        if is_train:\n",
    "            imgs = x_train\n",
    "            bboxes = y_train\n",
    "        else:\n",
    "            imgs = x_test\n",
    "            bboxes = y_test\n",
    "\n",
    "        # Read images and boxes and store them in a class attribute\n",
    "        images_list = []\n",
    "        bboxes_list = []\n",
    "        for img_name, labels in zip(imgs, bboxes):\n",
    "            img = cv2.imread(img_name)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(img)\n",
    "            boxes = np.loadtxt(labels, delimiter=\",\")\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            if len(boxes.shape) == 1:\n",
    "                boxes = boxes.unsqueeze(0)\n",
    "            image, boxes = self.resize(image=img, boxes=boxes)\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            # TODO: convert the images from xyxy to cxcywh\n",
    "            boxes = ...\n",
    "            images_list.append(image)\n",
    "            bboxes_list.append(boxes)\n",
    "        self.imgs = images_list\n",
    "        self.bboxes = bboxes_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and boxes from list\n",
    "        image = self.imgs[idx]\n",
    "        bboxes = self.bboxes[idx]\n",
    "\n",
    "        if self.transforms:  # Non geometric transforms\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Create a torch tensor with zeros with represent the labels of the boxes\n",
    "        # (There's only one class in this dataset)\n",
    "        labels = torch.zeros((len(bboxes),), dtype=torch.int64)\n",
    "\n",
    "        # TODO: add your code here\n",
    "        # Since the number of bounding boxes (aka leaves) per image is different, we need to\n",
    "        # create illegal boxes (with label=-1) so all images have the same number of boxes\n",
    "        # and we can create batches\n",
    "        # Remember, illegal_labels must have a dtype=torch.int64 and illegal_boxes a dtype=torch.float32\n",
    "        # 50 should be a good max number\n",
    "        illegal_needed = ...\n",
    "        illegal_labels = ...\n",
    "        illegal_boxes = ...\n",
    "\n",
    "        return image, {\n",
    "            \"labels\": torch.cat((labels, illegal_labels)),\n",
    "            \"boxes\": torch.cat((bboxes, illegal_boxes), axis=0),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: return the number of images in the dataset\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u79Feem38D6w"
   },
   "source": [
    "Now that we created our custom `Dataset` class, let's create an instance of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'leaf-dataset/detection'\n",
    "\n",
    "img_size = 384\n",
    "batch_size = 16\n",
    "\n",
    "augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = LeafDetectionDataset(\n",
    "    dataset_folder,\n",
    "    img_size=(img_size, img_size),\n",
    "    is_train=True,\n",
    "    transforms=augs,\n",
    ")\n",
    "\n",
    "display_imgs_bbox = []\n",
    "\n",
    "for i in range(10):\n",
    "    img, target = train_dataset[i]\n",
    "    img = ToPILImage()(img)\n",
    "    img = plot_bbox(img, target[\"boxes\"] * img_size)\n",
    "    display_imgs_bbox.append(img)\n",
    "\n",
    "# Plot two grids, one per list (don't forget the functions declared in the beginning of this notebook)\n",
    "plot_grid(imgs=display_imgs_bbox, nrows=2, ncols=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomendations\n",
    "This is a small dataset with roughly 100 images. The images present a low resolution and a big number of small objects. It's going to be a challenge for our simple Object Detection model. Don't be surprised if the average precision that you get oscilates between 0.4 and 0.6.\n",
    "\n",
    "Things to try for better performance:\n",
    "- Increase the resolution of the images. This also helps in the amount of objects that our detector can predict.\n",
    "- Use data augmentations.\n",
    "- Try different backbones/encoders.\n",
    "- Freeze the backbone/encoder so its weights are not trained. This is useful when dealing with small datasets. You can use the following code for it:\n",
    "    ```python\n",
    "    backbone = models.resnet50(pretrained=pretrained)\n",
    "    for param in backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    ```\n",
    "- We have a lot of overlapping objects. Therefore, be aware of the Non-Maximum suppression threshold that you use for both calculating the AP and making predictions.\n",
    "    - You can change the NMS threshold while calculating the AP during training like this: `ap = ap_calculator.calculate_map(model, nms_threshold=0.5)`\n",
    "    - And for predictions like this: `boxes, top_class, scores = predict(model, img, n_classes=1, nms_threshold=0.3)`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "leaf_detection_dataset_ANSWERS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
