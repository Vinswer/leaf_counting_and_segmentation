{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kHD5sjCFD1v",
    "outputId": "fcf42046-e36d-47db-8c73-0c2a13395314"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==0.16.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gjz4urLFilo"
   },
   "source": [
    "# The `Dataset` class\n",
    "\n",
    "A lot of effort in solving any machine learning problem goes in to preparing the data. The most important tool is the Dataset class. This class allows your deep learning algorithm to iterate over your data and apply different transormations or filters to it.\n",
    "\n",
    "For more info you can have a look at the [documentation](https://pytorch.org/docs/stable/data.html) of the Dataset class, or to its [source code](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py).\n",
    "\n",
    "To load your custom dataset, you need to create your own `CustomDataset` class, which should inherit the `torch.utils.data.Dataset` class. You'll always need to overwrite 3 metods:\n",
    "* `__init__` How your dataset should be initialized/created.\n",
    "* `__len__` To calculate the lenght of your dataset. This methods allows you to do `len(dataset)` and get the size of the dataset.\n",
    "* `__getitem__` To get one sample from your dataset based on its index. It supports the indexing such that `dataset[i]` can be used to get ith sample.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGNhDhMhI2Rf"
   },
   "source": [
    "# Building a custom dataset for leaf counting\n",
    "In this tutorial, you'll build a custom dataset cass which can be used to train a deep learning algorithm to count the number of leafs given an image of a plant.\n",
    "\n",
    "First, let's download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqPlSYVsJPGe",
    "outputId": "f64483c3-4de2-49f6-bb7a-51b048d991ed"
   },
   "outputs": [],
   "source": [
    "# Get dataset from git.wur.nl\n",
    "!git clone https://git.wur.nl/deep-learning-course/leaf-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrxlNELTLaBK",
    "outputId": "05f2176c-befd-4102-ca5a-47a3ccdf6034"
   },
   "outputs": [],
   "source": [
    "# Have a look at what the dataset contains\n",
    "!ls leaf-dataset/detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvZS3OF0Jc_X"
   },
   "source": [
    "## The Leaf Detection and counting Dataset\n",
    "This dataset contains labels for two different tasks: counting and detecting leafs.\n",
    "\n",
    "Concretely, it contains:\n",
    "* A RGB image per plant (e.g. *ara2012_plant117_rgb.png*)\n",
    "* A csv file per plant containing the bounding box of the leafs (e.g. *ara2012_plant117_bbox.csv*)\n",
    "* A csv file which store the total number of leafs per every plant in the dataset: *Leaf_counts.csv*\n",
    "### Regression task\n",
    "Since we want to create a dataset class that allows us to train a deep learning to count the number of leafs, we will need 2 things from the dataset:\n",
    "  1. The RGB image of every plant (both ara2012 and ara2013 sub-sets)\n",
    "  2. A csv files with the leaf counts per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBNm9RghMCsC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from d2l import torch as d2l\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL.Image import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xNd6d1GMDvw"
   },
   "outputs": [],
   "source": [
    "root = 'leaf-dataset/detection/' # Path to dataset\n",
    "\n",
    "# Take a look at one image\n",
    "image = d2l.Image.open(os.path.join(root, 'ara2012_plant001_rgb.png'))\n",
    "d2l.plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtxoW2mqMMHB"
   },
   "outputs": [],
   "source": [
    "# Check what the csv files contain\n",
    "with open(os.path.join(root, 'Leaf_counts.csv'), 'r') as f:\n",
    "    for line in f:\n",
    "        filename, n_leafs = (line.rstrip().split(', '))\n",
    "        print(filename, n_leafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e6_F57QNc23"
   },
   "source": [
    "## `LeafCountDataset`\n",
    "Let's start to create our custom dataset.\n",
    "\n",
    "To do so, first we need to create the class and the `__init__` method.\n",
    "```python\n",
    "class LeafDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "```\n",
    "\n",
    "**Exercise 1:** complete the `__init__` method so the `self.images` and `self.labels` attributes contain a list with all the images paths and counts of leafs. Do it in a way that `self.labels[i]` contains the number of leafs of the ith image `self.image[i]`. For instance:\n",
    "* `self.images[3] = 'leaf_segmentation_dataset/detection/ara2012_plant004_rgb.png'`\n",
    "* `self.labels[3] = 13`\n",
    "\n",
    "**Exercise 2:** now that the `__init__` method is complete, let's go for the `__get_item__` method. Try to complete the code in this method. To read an image given a path you might want to use the `d2l.Image.open` function. Additionally, the returned `labels` should be `torch.tensor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tX_JJQIFZgU"
   },
   "outputs": [],
   "source": [
    "class LeafDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory, is_train=True, transforms=None):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transforms = transforms\n",
    "  \n",
    "        with open(os.path.join(root, 'Leaf_counts.csv'), 'r') as f:\n",
    "            for line in f:\n",
    "                filename, n_leafs = (line.rstrip().split(', '))\n",
    "                img_path = os.path.join(directory, filename)\n",
    "                # TODO: add your code here (~2 lines). Fill the correspoinding lists with the images and labels\n",
    "\n",
    "      \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.images, self.labels, test_size=0.25, random_state=42)\n",
    "        if is_train:\n",
    "            self.images = X_train\n",
    "            self.labels = y_train\n",
    "        else:\n",
    "            self.images = X_test\n",
    "            self.labels = y_test\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: complete the code\n",
    "        # img = ...\n",
    "        # labels = ...\n",
    "        \n",
    "        # Assertions to check that everything is correct\n",
    "        assert(isinstance(img, Image)), \"Image variable should be a PIL Image\"\n",
    "        assert(isinstance(labels, torch.Tensor)), \"Labels varibable should be a torch tensor\"\n",
    "        assert(labels.dtype == torch.float32), \"Labels variable datatype should be float32\"\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W132gajUxr3"
   },
   "source": [
    "Let's check if our code works as it should. If you run the following block, you should see an image a a plant an a text saying how many leaves it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTcPEgqqP_f3"
   },
   "outputs": [],
   "source": [
    "# Let's check if the dataset class works properly\n",
    "dataset = LeafDataset(root, is_train=True)\n",
    "image, label = dataset[3]\n",
    "d2l.plt.imshow(image)\n",
    "print('This plant contains', int(label.detach().numpy()), 'leafs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzLKSz_aVLJp"
   },
   "source": [
    "Congratulations, you have built your first custom dataset class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "When you learned LeNet, you used this function to evaluate the performance of a network in a dataset:\n",
    "```python\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = d2l.Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_iter:        \n",
    "            if isinstance(features, list):\n",
    "               features = [feature.to(device) for feature in features]\n",
    "            else:\n",
    "               features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            metric.add(d2l.accuracy(net(features), labels), labels.numel())\n",
    "    return metric[0] / metric[1]\n",
    "```\n",
    "\n",
    "However, this fuction was built for classification, not for regression. We need to develop a corresponding one for regression which we can then use in our training loops. \n",
    "\n",
    "During the MLP lecture, you learned about Pearson correlation coefficient (also known as *r*). You can learn more about it [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). In summary, it evaluates the correlation between 2 set of values. The closer the value is to 1 or -1, the more correlated the values are. In our case, with predictions and ground truths, *r* tells you how close your network predictions are to the ground truth.\n",
    "```python\n",
    "def pearson_correlation(x1, x2, eps=1e-8):\n",
    "    \"\"\"Returns Pearson coefficient between 1D-tensors x1 and x2\n",
    "    Args:\n",
    "        x1 (Variable): First input (1D).\n",
    "        x2 (Variable): Second input (of size matching x1).\n",
    "        eps (float, optional): Small value to avoid division by zero.\n",
    "            Default: 1e-8\n",
    "    Example:\n",
    "        >>> input1 = autograd.Variable(torch.randn(128))\n",
    "        >>> input2 = autograd.Variable(torch.randn(128))\n",
    "        >>> output = F.pearson_correlation(input1, input2)\n",
    "        >>> print(output)\n",
    "    \"\"\"\n",
    "    assert x1.dim() == 1, \"Input must be 1D matrix / vector.\"\n",
    "    assert x1.size() == x2.size(), \"Input sizes must be equal.\"\n",
    "    x1_bar = x1 - x1.mean()\n",
    "    x2_bar = x2 - x2.mean()\n",
    "    dot_prod = x1_bar.dot(x2_bar)\n",
    "    norm_prod = x1_bar.norm(2) * x2_bar.norm(2)\n",
    "    return dot_prod / norm_prod.clamp(min=eps)\n",
    "```\n",
    "\n",
    "The loss itself can be used as a metric for our predictions. Therefore, we would like to calculate it together with the Pearson coefficient (*r*).\n",
    "\n",
    "Complete the code in the next block to develop this function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw this function in the MLP notebooks\n",
    "def pearson_correlation(x1, x2, eps=1e-8):\n",
    "    \"\"\"Returns Pearson coefficient between 1D-tensors x1 and x2\n",
    "    Args:\n",
    "        x1 (torch.Tensor): First input (1D).\n",
    "        x2 (torch.Tensor): Second input (of size matching x1).\n",
    "        eps (float, optional): Small value to avoid division by zero.\n",
    "            Default: 1e-8\n",
    "    Example:\n",
    "        >>> input1 = autograd.Variable(torch.randn(128))\n",
    "        >>> input2 = autograd.Variable(torch.randn(128))\n",
    "        >>> output = F.pearson_correlation(input1, input2)\n",
    "        >>> print(output)\n",
    "    \"\"\"\n",
    "    assert x1.dim() == 1, \"Input must be 1D matrix / vector.\"\n",
    "    assert x1.size() == x2.size(), \"Input sizes must be equal.\"\n",
    "    x1_bar = x1 - x1.mean()\n",
    "    x2_bar = x2 - x2.mean()\n",
    "    dot_prod = x1_bar.dot(x2_bar)\n",
    "    norm_prod = x1_bar.norm(2) * x2_bar.norm(2)\n",
    "    return dot_prod / norm_prod.clamp(min=eps)\n",
    "\n",
    "\n",
    "def evaluate_loss_pearson_gpus(net, data_iter, loss, device):\n",
    "    '''\n",
    "    Function to evaluate the loss and Pearson coefficient of a CNN on a data iterator\n",
    "    Args:\n",
    "        net: network\n",
    "        data_iter (torch.data.Dataloader): dataloader to iterate to\n",
    "        loss: loss used to train the model\n",
    "        device: device where the model is loaded. For example, gpu0. You can get it from d2l.try_all_gpus()\n",
    "    \n",
    "    Returns:\n",
    "        average loss of the model in the data\n",
    "        average pearson coefficient in the data\n",
    "    \n",
    "    '''\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    metric = d2l.Accumulator(4)  # loss, num_examples, pearson, 1 (for loop iter counter)\n",
    "    for features, labels in data_iter:        \n",
    "        if isinstance(features, list):\n",
    "            features = [feature.to(device) for feature in features]\n",
    "        else:\n",
    "            features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = net(features)\n",
    "        # TODO: add your code here (~2 lines). \n",
    "        # Expected output: variable called loss_sum which contains the sum of the losses per sample\n",
    "        # ...\n",
    "        # loss_sum = ...\n",
    "\n",
    "        # Check that loss_sum is a float\n",
    "        assert(isinstance(loss_sum, float)), \"loss variable variable should be a float type\"\n",
    "        \n",
    "        # TODO: add your code here (~2 lines). \n",
    "        # Expected output: variable called pr which contains pearson coefficient of the samples and GT\n",
    "        # ...\n",
    "        # pr = ...\n",
    "        \n",
    "        # Check that pr is a float\n",
    "        assert(isinstance(pr, float)), \"Pearson coefficient variable should be a float type\"\n",
    "        metric.add(loss_sum, pr, labels.shape[0], 1)\n",
    "    return metric[0] / metric[2], metric[1] / metric[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start with the first part of the project!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "regression_dataset_ANSWERS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
